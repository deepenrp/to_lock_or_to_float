{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d053d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from functools import reduce\n",
    "import yfinance as yf\n",
    "\n",
    "import datetime as dt\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Import PlotLy Dependencies\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c800b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following Jupyter Notebooks first\n",
    "%run data_source.ipynb\n",
    "%run inflation.ipynb\n",
    "%run productivity.ipynb\n",
    "%run jobs.ipynb\n",
    "%run housing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a7798",
   "metadata": {},
   "source": [
    "# Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Inflation Data\n",
    "inflation_change_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee787ce0",
   "metadata": {},
   "source": [
    "# Productivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c202fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Productivity Data. Notice that the data is lagging. For this reason, we will use only the GDP number for productivity.\n",
    "productivity_index_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14aea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided to just use the GDP data only for productivity\n",
    "gdp_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef95dfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0593c7c",
   "metadata": {},
   "source": [
    "# Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4706e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Jobs Data\n",
    "jobs_index_change_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362460",
   "metadata": {},
   "source": [
    "# Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b04885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Housing Data. Notice that the data is lagging. For this reason, we will use a 12 month moving average to bring the data to latest date that will align with other dataframes.\n",
    "housing_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_housing_monthly_data = pd.DataFrame()\n",
    "\n",
    "dates_list = list(housing_monthly_data[\"Date\"])\n",
    "i_list = list(housing_monthly_data[\"Overall Housing Index\"])[-12:]\n",
    "\n",
    "change_list = list(housing_monthly_data[\"Change in Overall Housing Index (%)\"])\n",
    "\n",
    "# last_change = list(housing_monthly_data[\"Housing Change in Velocity (basis points)\"])[-1]\n",
    "velocity_list = []\n",
    "\n",
    "for i in range(5):\n",
    "    # New Dates\n",
    "    dates_list.append(dates_list[-1] + pd.DateOffset(months=1))\n",
    "    \n",
    "    # Index Averages to add\n",
    "    avg = sum(i_list[-12:]) / 12\n",
    "    \n",
    "    # Change (%) \n",
    "    change = ((avg - i_list[-1]) / i_list[-1]) * 100  \n",
    "    i_list.append(avg)\n",
    "    \n",
    "    # Velocity\n",
    "    velocity = (change - change_list[-1]) * 100\n",
    "    \n",
    "    change_list.append(change)\n",
    "    velocity_list.append(velocity)\n",
    "        \n",
    "\n",
    "fabricated_index_list = i_list[-5:]\n",
    "\n",
    "updated_housing_monthly_data[\"Date\"] = dates_list\n",
    "\n",
    "indexes = list(housing_monthly_data[\"Overall Housing Index\"]) + fabricated_index_list\n",
    "updated_housing_monthly_data[\"Overall Housing Index\"] = indexes\n",
    "\n",
    "changes = list(housing_monthly_data[\"Change in Overall Housing Index (%)\"]) + change_list\n",
    "\n",
    "updated_housing_monthly_data[\"Change in Overall Housing Index (%)\"] = change_list\n",
    "\n",
    "      \n",
    "velocities = list(housing_monthly_data[\"Housing Change in Velocity (basis points)\"]) + velocity_list\n",
    "updated_housing_monthly_data[\"Housing Change in Velocity (basis points)\"] = velocities\n",
    "\n",
    "\n",
    "updated_housing_monthly_data.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4a3f11",
   "metadata": {},
   "source": [
    "# Interest Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cf3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GS10 - Market Yield on U.S. Treasury Securities at 10-Year Constant Maturity. This will be used to gauge rates\n",
    "interest_rate_df = pd.DataFrame(fred_api_function(\"GS10\"))\n",
    "\n",
    "# Rename the 'Value' column to '10-Year Real Interest Rate'\n",
    "interest_rate_df.rename(columns={'Value': '10-Year Treasury Yield'}, inplace=True)\n",
    "\n",
    "interest_rate_df[\"10-Yr Yield (basis points)\"] = interest_rate_df[\"10-Year Treasury Yield\"] * 100\n",
    "\n",
    "interest_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9133c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Dates to the appropriate data type\n",
    "inflation_change_df[\"Date\"] = pd.to_datetime(inflation_change_df[\"Date\"])\n",
    "gdp_monthly_data[\"Date\"] = pd.to_datetime(gdp_monthly_data[\"Date\"])\n",
    "jobs_index_change_df[\"Date\"] = pd.to_datetime(jobs_index_change_df[\"Date\"])\n",
    "updated_housing_monthly_data[\"Date\"] = pd.to_datetime(updated_housing_monthly_data[\"Date\"])\n",
    "interest_rate_df[\"Date\"] = pd.to_datetime(interest_rate_df[\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Merge Inflation and Productivity DataFrames on the 'Date' column\n",
    "combined_df = pd.merge(inflation_change_df, gdp_monthly_data, on='Date')\n",
    "\n",
    "# Merge Combined DataFrames to Jobs DataFrame on the 'Date' column\n",
    "combined_df = pd.merge(combined_df, jobs_index_change_df, on='Date')\n",
    "\n",
    "# Merge Combined DataFrames to Housing DataFrame on the 'Date' column\n",
    "combined_df = pd.merge(combined_df, updated_housing_monthly_data, on='Date')\n",
    "\n",
    "# Merge Combined DataFrames to Interest Rate DataFrame on the 'Date' column\n",
    "combined_df = pd.merge(combined_df, interest_rate_df, on='Date')\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce columns of the DataFrame to only the relevant columns\n",
    "\n",
    "selected_columns = [\"Date\", \"Inflation Change in Velocity (basis points)\", \"GDP Change in Velocity (basis points)\", \\\n",
    "                   \"Jobs Index Change in Velocity (basis points)\", \"Housing Change in Velocity (basis points)\", \\\n",
    "                   \"10-Yr Yield (basis points)\"]\n",
    "\n",
    "\n",
    "reduced_df = combined_df[selected_columns]\n",
    "\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lock or Float Historical Lock/Float Best Decisions\n",
    "yield_list = reduced_df[\"10-Yr Yield (basis points)\"]\n",
    "\n",
    "lock_yn_list = []\n",
    "\n",
    "def generate_lock_decisions(yields):\n",
    "    # Initialize an empty list to store the output\n",
    "    output = []\n",
    "\n",
    "    # Iterate through the yields starting from the second element\n",
    "    for i in range(0, len(yields)):\n",
    "        try:\n",
    "            # Check if the current yield is higher than the previous one\n",
    "            if yields[i] < yields[i + 1]:\n",
    "                output.append(1)\n",
    "            else:\n",
    "                output.append(0)\n",
    "        except:\n",
    "            output.append(\"predict this\")\n",
    "\n",
    "    return output\n",
    "\n",
    "lock_yn_list = generate_lock_decisions(yield_list)\n",
    "\n",
    "reduced_df[\"lock_yn\"] = lock_yn_list\n",
    "\n",
    "# Notice the \"predict this\" on the last row to remind us to what decision we are trying to determine\n",
    "reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop N/As in the top 6 rows\n",
    "dropped_df = reduced_df.iloc[6:]\n",
    "dropped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858be6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Last Row to start building the ML model\n",
    "dropped_last_row_df = dropped_df.iloc[:-1]\n",
    "dropped_last_row_df\n",
    "# Drop first column\n",
    "dropped_first_column_df = dropped_last_row_df.iloc[:, 1:]\n",
    "dropped_first_column_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5bdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Data\n",
    "features = dropped_first_column_df.iloc[:, :-1]\n",
    "features_data_matrix = features.values\n",
    "print(len(features_data_matrix))\n",
    "# Decision Labels\n",
    "labels = dropped_first_column_df.iloc[:, -1]\n",
    "labels_data = labels.values.astype(int)\n",
    "print(len(labels_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e61a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01dd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d12be",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050309cb",
   "metadata": {},
   "source": [
    "Determine Independent variables (X) and Dependent variables (y). Then split the data into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba89eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = features_data_matrix\n",
    "y = labels_data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, stratify=y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(X)\n",
    "shuffled_X = np.array(X).reshape((431, 5))\n",
    "# shuffled_X\n",
    "\n",
    "random.shuffle(y)\n",
    "shuffled_y = np.array(y).reshape((431,))\n",
    "# shuffled_y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(shuffled_X, shuffled_y, random_state=5, stratify=shuffled_y)\n",
    "print(shuffled_X.shape, shuffled_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32133e",
   "metadata": {},
   "source": [
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the new independent variables to predict decision\n",
    "new_data = np.array([[-42.802841, 118.172447, 64.554294, -1.073848, 438.0 ]])\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b6730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict the decision of the new data point\n",
    "predictions = classifier.predict(new_data)\n",
    "print(\"Classes are either 0 (Float) or 1 (Lock)\")\n",
    "print(f\"The new point was classified as: {predictions}\")\n",
    "print(\"This means that it is predicting that the Treasury yield will go down and to float. Floating is suggested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would the model predict the test data\n",
    "test_predictions = classifier.predict(X_test)\n",
    "results_df = pd.DataFrame({\"Prediction\": test_predictions, \"Actual\": y_test})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792fe21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_yn = []\n",
    "for index, row in results_df.iterrows():\n",
    "    if row[0] == row[1]:\n",
    "        match_yn.append(1)\n",
    "    else:\n",
    "        match_yn.append(0)\n",
    "        \n",
    "results_df[\"match_yn\"] = match_yn\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of matched\n",
    "count_results = results_df['match_yn'].value_counts()[1]\n",
    "percentage = (count_results / len(results_df['match_yn'])) * 100\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422ac44",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649de2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(dropped_first_column_df.columns[:5])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd24c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f059093",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f874aa58",
   "metadata": {},
   "source": [
    "###### We can use a random forest classifier to determine how important each feature is to the classify the lock_yn decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7841041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would a random forest classifier score?\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf = rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f5edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests in sklearn will automatically calculate feature importance\n",
    "importances = rf.feature_importances_\n",
    "features_list = ['Monthly Change in Inflation Index (%)', 'Monthly Change in GDP (%)', 'Monthly Change in Jobs Index (%)', \\\n",
    "            'Monthly Change in Overall Housing Index (%)']\n",
    "\n",
    "# We can sort the features by their importance\n",
    "sorted(zip(importances, features_list), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2bbbe",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf505ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create the SVC Model (Support Vector Classifier)\n",
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4976e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c714d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the grid search estimator. \n",
    "# This will take the SVC model and try each combination of parameters\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best score\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f7d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08528d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"Lock\", \"Float\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1597fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec10e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f832c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_yn1 = []\n",
    "for index, row in res_df.iterrows():\n",
    "    if row[0] == row[1]:\n",
    "        match_yn1.append(1)\n",
    "    else:\n",
    "        match_yn1.append(0)\n",
    "        \n",
    "res_df[\"match_yn\"] = match_yn1\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee98b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of matched\n",
    "ct_results = res_df['match_yn'].value_counts()[1]\n",
    "perc = (ct_results / len(res_df['match_yn'])) * 100\n",
    "perc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594a0af",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034d5152",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c44950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6ae64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 5)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "plt.plot(explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489f2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0737a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a720ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfb4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test})\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_yn_1 = []\n",
    "for index, row in pca_df.iterrows():\n",
    "    if row[0] == row[1]:\n",
    "        match_yn_1.append(1)\n",
    "    else:\n",
    "        match_yn_1.append(0)\n",
    "        \n",
    "pca_df[\"match_yn\"] = match_yn_1\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of matched\n",
    "count_results_1 = pca_df[\"match_yn\"].value_counts()[1]\n",
    "percentage_1 = (count_results / len(pca_df['match_yn'])) * 100\n",
    "percentage_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4990126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9659e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred = classifier.predict(new_data)\n",
    "new_pred\n",
    "# 0 = Float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4516102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0fb5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a7f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
